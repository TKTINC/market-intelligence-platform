# services/llama-explanation/src/monitoring.py
import logging
import time
from typing import Dict, Any, Optional
import prometheus_client
from prometheus_client import Counter, Histogram, Gauge, Info

logger = logging.getLogger(__name__)

# Prometheus metrics for Llama service
LLAMA_REQUESTS_TOTAL = Counter(
    'llama_requests_total',
    'Total requests to Llama service',
    ['priority', 'status']
)

LLAMA_PROCESSING_DURATION = Histogram(
    'llama_processing_duration_seconds',
    'Time spent processing Llama requests',
    ['priority']
)

LLAMA_TOKENS_GENERATED = Counter(
    'llama_tokens_generated_total',
    'Total tokens generated by Llama'
)

LLAMA_QUEUE_DEPTH = Gauge(
    'llama_queue_depth',
    'Current queue depth for Llama requests'
)

LLAMA_ACTIVE_REQUESTS = Gauge(
    'llama_active_requests',
    'Currently active Llama requests'
)

LLAMA_MODEL_LOAD_TIME = Gauge(
    'llama_model_load_time_seconds',
    'Time taken to load Llama model'
)

LLAMA_GPU_MEMORY_USAGE = Gauge(
    'llama_gpu_memory_usage_bytes',
    'GPU memory usage in bytes'
)

LLAMA_TOKENS_PER_SECOND = Gauge(
    'llama_tokens_per_second',
    'Average tokens generated per second'
)

LLAMA_CONFIDENCE_SCORE = Histogram(
    'llama_confidence_score',
    'Confidence scores of generated explanations'
)

# Service info
LLAMA_SERVICE_INFO = Info(
    'llama_service_info',
    'Information about the Llama service'
)

class LlamaMetrics:
    """Metrics collector for Llama service"""
    
    def __init__(self):
        self.start_time = time.time()
        self.request_count = 0
        self.total_tokens = 0
        self.total_processing_time = 0.0
        
        # Set service info
        LLAMA_SERVICE_INFO.info({
            'version': '1.0.0',
            'model': 'llama-2-7b-explanations',
            'quantization': 'Q4_K_M',
            'service': 'llama-explanation'
        })
        
        logger.info("Llama metrics collector initialized")
    
    def record_request(self, priority: str = "normal"):
        """Record a new request"""
        self.request_count += 1
        LLAMA_REQUESTS_TOTAL.labels(priority=priority, status='started').inc()
    
    def record_success(
        self,
        processing_time_ms: int,
        tokens_generated: int,
        priority: str = "normal",
        confidence_score: float = 0.0
    ):
        """Record successful request completion"""
        
        processing_time_s = processing_time_ms / 1000.0
        
        # Update counters and histograms
        LLAMA_REQUESTS_TOTAL.labels(priority=priority, status='success').inc()
        LLAMA_PROCESSING_DURATION.labels(priority=priority).observe(processing_time_s)
        LLAMA_TOKENS_GENERATED.inc(tokens_generated)
        LLAMA_CONFIDENCE_SCORE.observe(confidence_score)
        
        # Update internal stats
        self.total_tokens += tokens_generated
        self.total_processing_time += processing_time_s
        
        # Update tokens per second
        if self.total_processing_time > 0:
            tokens_per_second = self.total_tokens / self.total_processing_time
            LLAMA_TOKENS_PER_SECOND.set(tokens_per_second)
    
    def record_error(self, error_message: str, priority: str = "normal"):
        """Record request error"""
        LLAMA_REQUESTS_TOTAL.labels(priority=priority, status='error').inc()
        logger.error(f"Request failed: {error_message}")
    
    def record_timeout(self, priority: str = "normal"):
        """Record request timeout"""
        LLAMA_REQUESTS_TOTAL.labels(priority=priority, status='timeout').inc()
    
    def update_queue_depth(self, depth: int):
        """Update queue depth metric"""
        LLAMA_QUEUE_DEPTH.set(depth)
    
    def update_active_requests(self, count: int):
        """Update active requests metric"""
        LLAMA_ACTIVE_REQUESTS.set(count)
    
    def record_model_load_time(self, load_time_seconds: float):
        """Record model loading time"""
        LLAMA_MODEL_LOAD_TIME.set(load_time_seconds)
    
    def update_gpu_memory_usage(self, usage_bytes: int):
        """Update GPU memory usage"""
        LLAMA_GPU_MEMORY_USAGE.set(usage_bytes)
    
    def get_summary(self) -> Dict[str, Any]:
        """Get metrics summary"""
        uptime = time.time() - self.start_time
        
        return {
            'uptime_seconds': uptime,
            'total_requests': self.request_count,
            'total_tokens_generated': self.total_tokens,
            'average_tokens_per_second': (
                self.total_tokens / self.total_processing_time 
                if self.total_processing_time > 0 else 0
            ),
            'total_processing_time': self.total_processing_time
        }

def setup_metrics():
    """Setup Prometheus metrics endpoint"""
    try:
        from config import settings
        
        if settings.PROMETHEUS_ENABLED:
            # Start Prometheus metrics server
            prometheus_client.start_http_server(settings.METRICS_PORT)
            logger.info(f"Prometheus metrics server started on port {settings.METRICS_PORT}")
        
        return True
        
    except Exception as e:
        logger.error(f"Failed to start Prometheus metrics server: {str(e)}")
        return False

# Health check utilities
def check_gpu_health() -> Dict[str, Any]:
    """Check GPU health and memory usage"""
    try:
        import torch
        
        if not torch.cuda.is_available():
            return {
                'gpu_available': False,
                'error': 'CUDA not available'
            }
        
        device_count = torch.cuda.device_count()
        current_device = torch.cuda.current_device()
        device_name = torch.cuda.get_device_name(current_device)
        
        # Memory information
        memory_allocated = torch.cuda.memory_allocated(current_device)
        memory_reserved = torch.cuda.memory_reserved(current_device)
        memory_total = torch.cuda.get_device_properties(current_device).total_memory
        
        return {
            'gpu_available': True,
            'device_count': device_count,
            'current_device': current_device,
            'device_name': device_name,
            'memory': {
                'allocated_mb': memory_allocated / (1024**2),
                'reserved_mb': memory_reserved / (1024**2),
                'total_mb': memory_total / (1024**2),
                'utilization_percent': (memory_allocated / memory_total) * 100
            }
        }
        
    except Exception as e:
        return {
            'gpu_available': False,
            'error': str(e)
        }

def check_model_health(llama_engine) -> Dict[str, Any]:
    """Check Llama model health"""
    try:
        if not llama_engine or not llama_engine.is_ready():
            return {
                'model_loaded': False,
                'error': 'Model not loaded or not ready'
            }
        
        # Get model statistics
        stats = llama_engine.generation_stats
        
        return {
            'model_loaded': True,
            'model_ready': llama_engine.is_ready(),
            'total_requests': stats.get('total_requests', 0),
            'total_tokens_generated': stats.get('total_tokens_generated', 0),
            'average_tokens_per_second': stats.get('average_tokens_per_second', 0.0)
        }
        
    except Exception as e:
        return {
            'model_loaded': False,
            'error': str(e)
        }

# Performance monitoring utilities
class PerformanceMonitor:
    """Monitor and log performance metrics"""
    
    def __init__(self, log_interval: int = 60):
        self.log_interval = log_interval
        self.last_log_time = time.time()
        self.metrics_history = []
    
    def should_log(self) -> bool:
        """Check if it's time to log metrics"""
        return time.time() - self.last_log_time >= self.log_interval
    
    def log_performance(self, metrics: Dict[str, Any]):
        """Log performance metrics"""
        if self.should_log():
            self.metrics_history.append({
                'timestamp': time.time(),
                'metrics': metrics.copy()
            })
            
            # Keep only last 24 hours of data
            cutoff_time = time.time() - (24 * 3600)
            self.metrics_history = [
                entry for entry in self.metrics_history
                if entry['timestamp'] > cutoff_time
            ]
            
            # Log current metrics
            logger.info(f"Performance metrics: {metrics}")
            self.last_log_time = time.time()
    
    def get_performance_trend(self, metric_name: str, hours: int = 1) -> List[float]:
        """Get performance trend for a specific metric"""
        cutoff_time = time.time() - (hours * 3600)
        
        values = []
        for entry in self.metrics_history:
            if entry['timestamp'] > cutoff_time and metric_name in entry['metrics']:
                values.append(entry['metrics'][metric_name])
        
        return values

# Example usage
if __name__ == "__main__":
    # Test metrics setup
    setup_metrics()
    
    # Test metrics collection
    metrics = LlamaMetrics()
    
    # Simulate some requests
    metrics.record_request("high")
    metrics.record_success(500, 150, "high", 0.89)
    
    metrics.record_request("normal")
    metrics.record_success(300, 100, "normal", 0.85)
    
    # Get summary
    summary = metrics.get_summary()
    print(f"Metrics summary: {summary}")
    
    # Test health checks
    gpu_health = check_gpu_health()
    print(f"GPU health: {gpu_health}")
