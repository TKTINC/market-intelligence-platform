# services/llama-explanation/requirements.txt
# Core framework
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0

# HTTP and async
httpx==0.25.0
aiofiles==23.2.1

# GPU and ML
torch==2.1.0+cu121 --extra-index-url https://download.pytorch.org/whl/cu121
# llama-cpp-python will be installed separately with CUDA support

# Monitoring and utilities
prometheus-client==0.19.0
psutil==5.9.6

# Development and testing
pytest==7.4.3
pytest-asyncio==0.21.1
